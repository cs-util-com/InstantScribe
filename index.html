<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <title>Audio Recorder</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="icon" href="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'%3E%3Ccircle cx='50' cy='50' r='45' fill='%23ff0000'/%3E%3C/svg%3E" type="image/svg+xml">
  <!-- Add lamejs library -->
  <script src="https://cdn.jsdelivr.net/npm/lamejs@1.2.1/lame.min.js"></script>
  <style>
    /* Base styles for dark theme */
    body {
      font-family: Consolas, monospace;
      margin: 0;
      padding: 10px;
      background: #121212;
      color: #e0e0e0;
    }

    .container {
      max-width: 800px;
      margin: 0 auto;
      background: #1e1e1e;
      padding: 15px;
      border-radius: 8px;
      box-shadow: 0 2px 5px rgba(0, 0, 0, 0.5);
    }

    h3 {
      margin: 10px 0;
      font-size: 1.2rem;
      color: #ffffff;
    }

    /* Controls layout */
    .controls {
      display: flex;
      flex-wrap: wrap;
      align-items: center;
      gap: 10px;
      margin-bottom: 15px;
    }

    .control-group {
      display: flex;
      align-items: center;
      gap: 5px;
    }

    input[type="text"],
    select {
      padding: 6px 8px;
      font-size: 1rem;
      background: #333;
      border: 1px solid #555;
      border-radius: 4px;
      color: #e0e0e0;
    }

    button {
      padding: 8px 12px;
      background-color: #007bff;
      color: #fff;
      border: none;
      border-radius: 4px;
      cursor: pointer;
      font-size: 1rem;
    }

    button:hover {
      background-color: #0056b3;
    }

    /* Transcription box */
    #transcription-container {
      margin-top: 15px;
      padding: 10px;
      border: 1px solid #444;
      border-radius: 4px;
      white-space: pre-wrap;
      font-family: Consolas, monospace;
      font-size: 0.9rem;
      background: #2c2c2c;
      color: #e0e0e0;
    }

    /* Responsive adjustments for small screens */
    @media (max-width: 600px) {
      .controls {
        flex-direction: column;
        align-items: stretch;
      }

      .control-group,
      button {
        width: 100%;
      }
    }

    /* Notification banner */
    .notification {
      position: fixed;
      bottom: 20px;
      left: 50%;
      transform: translateX(-50%);
      background-color: rgba(0, 0, 0, 0.7);
      color: white;
      padding: 10px 15px;
      border-radius: 5px;
      z-index: 1000;
      text-align: center;
      transition: opacity 0.5s;
    }
  </style>
</head>

<body>
  <div class="container">
    <div class="controls">
      <input type="text" id="custom-filename" placeholder="File title...">
      <button id="stop">Save Recording</button>
      <div class="control-group">
        <label for="language-select">Transcribe in </label>
        <select id="language-select">
          <option value="en-US">English (US)</option>
          <option value="en-GB">English (UK)</option>
          <option value="de-DE">German</option>
          <option value="fr-FR">French</option>
          <option value="es-ES">Spanish</option>
          <option value="it-IT">Italian</option>
          <option value="ja-JP">Japanese</option>
          <option value="zh-CN">Chinese (Simplified)</option>
        </select>
      </div>
    </div>

    <h3>Transcription:</h3>
    <div id="transcription-container">
      <!-- The transcription element is contenteditable (editable by the user) -->
      <div id="transcription" contenteditable="true"></div>
    </div>
  </div>

  <script>
    // Global variable to track recording state
    let isRecording = false;
    let exitWarningEnabled = false;
    let mp3Encoder = null;
    let mp3EncoderSampleRate = 44100; // Default sample rate for MP3 encoder

    // Check if the browser supports the required APIs
    if (!window.MediaRecorder || !(window.webkitSpeechRecognition || window.SpeechRecognition)) {
      alert("Your browser does not support the required APIs for audio recording and transcription.");
    } else {
      // Show notification to encourage user interaction
      const notification = document.createElement('div');
      notification.className = 'notification';
      notification.innerHTML = 'Recording & Transcription started (Click anywhere to hide message)';
      document.body.appendChild(notification);
      
      // Set up one-time click handler to enable exit warnings
      document.addEventListener('click', function enableExitWarning() {
        exitWarningEnabled = true;
        document.removeEventListener('click', enableExitWarning);
        // Fade out and remove notification
        notification.style.opacity = '0';
        setTimeout(() => notification.remove(), 500);
      }, { once: true });

      navigator.mediaDevices.getUserMedia({ audio: true }).then(stream => {
        // Use a single timestamp for the entire recording session.
        const recordingStartTime = Date.now();
        // Grab DOM elements for use in event handlers.
        const transcriptionElement = document.getElementById('transcription'),
              languageSelect = document.getElementById('language-select'),
              stopButton = document.getElementById('stop'),
              customFilenameInput = document.getElementById('custom-filename');

        // Determine if MP3 recording is supported natively
        const isMP3Supported = MediaRecorder.isTypeSupported('audio/mpeg') || MediaRecorder.isTypeSupported('audio/mp3');
        const mimeType = isMP3Supported ? 
                         (MediaRecorder.isTypeSupported('audio/mpeg') ? 'audio/mpeg' : 'audio/mp3') : 
                         ['audio/ogg', 'audio/mp4', 'audio/webm'].find(type => MediaRecorder.isTypeSupported(type)) || '';
        
        // Always use MP3 as the file extension, even if we'll be encoding manually
        const fileExtension = 'mp3';
        
        let audioProcessor = null;
        let audioContext = null;
        let audioChunks = [];
        let mediaRecorder = null;

        // If MP3 is natively supported, use MediaRecorder
        if (isMP3Supported) {
          // Speech-optimized settings
          const speechBitrate = 48000; // 48kbps bitrate - good for speech quality with smaller file size
          mediaRecorder = new MediaRecorder(stream, { mimeType, audioBitsPerSecond: speechBitrate });
          mediaRecorder.ondataavailable = event => { 
            if (event.data.size > 0) audioChunks.push(event.data); 
          };
          mediaRecorder.onstop = () => {
            const customTitle = customFilenameInput.value.trim(),
                  audioFileName = `${getDateTimePrefix(recordingStartTime)}${customTitle ? ' ' + customTitle : ''} audio recording.${fileExtension}`,
                  audioBlob = new Blob(audioChunks, { type: 'audio/mpeg' });
            downloadBlob(audioBlob, audioFileName);
          };
        } 
        // Otherwise use Web Audio API with lamejs
        else {
          audioContext = new (window.AudioContext || window.webkitAudioContext)();
          const sourceNode = audioContext.createMediaStreamSource(stream);
          
          // Create a ScriptProcessorNode to process audio
          // Smaller buffer size (8192) for more efficient encoding
          audioProcessor = audioContext.createScriptProcessor(8192, 1, 1);
          let mp3Data = [];
          
          // Initialize MP3 encoder with speech-optimized settings
          mp3EncoderSampleRate = audioContext.sampleRate;
          mp3Encoder = new lamejs.Mp3Encoder(1, mp3EncoderSampleRate, speechBitrate / 1000);
          
          audioProcessor.onaudioprocess = (e) => {
            if (!isRecording) return;
            
            // Get audio data from input channel
            const samples = e.inputBuffer.getChannelData(0);
            
            // Convert float32 to int16
            const sampleBuffer = new Int16Array(samples.length);
            for (let i = 0; i < samples.length; i++) {
              // Scale to int16 range and clamp to avoid overflow
              sampleBuffer[i] = Math.max(-32768, Math.min(32767, samples[i] * 32768));
            }
            
            // Encode to MP3 with speech-optimized settings
            const mp3buf = mp3Encoder.encodeBuffer(sampleBuffer);
            if (mp3buf.length > 0) {
              mp3Data.push(mp3buf);
            }
          };
          
          // Connect the nodes
          sourceNode.connect(audioProcessor);
          audioProcessor.connect(audioContext.destination);
          
          // Handle stop for manual MP3 encoding
          stopButton.addEventListener('click', function stopMP3Recording() {
            if (audioProcessor) {
              // Finalize the MP3 encoding
              const mp3Final = mp3Encoder.flush();
              if (mp3Final.length > 0) {
                mp3Data.push(mp3Final);
              }
              
              // Create blob from MP3 data
              const blob = new Blob(mp3Data, { type: 'audio/mpeg' });
              const customTitle = customFilenameInput.value.trim();
              const audioFileName = `${getDateTimePrefix(recordingStartTime)}${customTitle ? ' ' + customTitle : ''} audio recording.${fileExtension}`;
              
              // Download the MP3 file
              downloadBlob(blob, audioFileName);
              
              // Clean up audio processing
              sourceNode.disconnect();
              audioProcessor.disconnect();
              audioProcessor = null;
              
              // Only remove this specific listener to avoid interfering with other stop button functionality
              stopButton.removeEventListener('click', stopMP3Recording);
            }
          });
        }

        // Start recording using the appropriate method
        if (isMP3Supported) {
          mediaRecorder.start();
        } else {
          // For lamejs method, recording starts as soon as we set up the audio processor
          console.log("Using lamejs for MP3 encoding (native MP3 recording not supported)");
        }

        // Set up Speech Recognition for live transcription.
        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition,
              recognition = new SpeechRecognition();
        recognition.continuous = true;
        recognition.interimResults = true;

        function getSupportedLanguage(lang) {
          // Get the list of supported languages from the dropdown.
          const availableLanguages = Array.from(languageSelect.options).map(option => option.value);
          // If the language exactly matches one of the options, return it.
          if (availableLanguages.includes(lang)) {
            return lang;
          }
          // Otherwise, try to match based on the two-letter code (e.g. "de" -> "de-DE")
          const twoLetter = lang.split('-')[0];
          const match = availableLanguages.find(l => l.startsWith(twoLetter));
          return match || 'en-US';
        }

        const browserLang = navigator.language || 'en-US';
        const selectedLang = getSupportedLanguage(browserLang);
        recognition.lang = selectedLang;
        languageSelect.value = selectedLang;
        
        // Set isRecording to true - now in global scope
        isRecording = true;
        let interimStartTime = null; // Tracks when an interim transcript begins.
        const interimSpanId = 'interim-transcript';

        recognition.onresult = event => {
          let finalTranscript = '', interimTranscript = '';
          for (let i = event.resultIndex; i < event.results.length; i++) {
            const result = event.results[i][0].transcript;
            event.results[i].isFinal ? finalTranscript += result : interimTranscript += result;
          }

          // If an interim result begins, note its start time.
          if (interimTranscript && !interimStartTime) interimStartTime = Date.now();

          // When a final result is available, insert it along with a timestamp.
          if (finalTranscript) {
            removeInterimTranscript();
            if (interimStartTime) {
              const elapsed = interimStartTime - recordingStartTime,
                    minutes = Math.floor(elapsed / 60000),
                    seconds = Math.floor((elapsed % 60000) / 1000),
                    timestampDiv = document.createElement('div');
              timestampDiv.style.fontWeight = 'bold';
              timestampDiv.style.color = 'lightgray';
              timestampDiv.textContent = `[[${padZero(minutes)}:${padZero(seconds)}]] `;
              transcriptionElement.appendChild(timestampDiv);
              interimStartTime = null;
            }
            transcriptionElement.appendChild(document.createTextNode(finalTranscript));
          }

          // Always update or add the interim transcript.
          if (interimTranscript) {
            removeInterimTranscript();
            const interimSpan = document.createElement('span');
            interimSpan.id = interimSpanId;
            interimSpan.style.opacity = '0.5';
            interimSpan.textContent = interimTranscript;
            transcriptionElement.appendChild(interimSpan);
          }
        };

        recognition.onerror = event => {
          console.error('Speech recognition error:', event.error);
          // Attempt to restart if still recording.
          if (isRecording) {
            recognition.stop();
            recognition.start();
          }
        };
        recognition.onend = () => { if (isRecording) recognition.start(); };
        recognition.start();

        // Update language if the user selects a new option.
        languageSelect.addEventListener('change', () => {
          recognition.stop();
          recognition.lang = languageSelect.value;
          if (isRecording) recognition.start();
        });

        // When the user clicks the stop button, stop both audio and transcription.
        stopButton.addEventListener('click', () => {
          // For MediaRecorder method
          if (mediaRecorder && isRecording) {
            mediaRecorder.stop();
          }
          
          recognition.stop();
          stopButton.style.display = 'none';
          isRecording = false; // Now updates the global variable

          // Remove any lingering interim transcript.
          removeInterimTranscript();

          // Retrieve the (possibly edited) transcription.
          const transcriptionHTML = transcriptionElement.innerHTML,
                transcriptionText = htmlToPlainText(transcriptionHTML),
                customTitle = customFilenameInput.value.trim(),
                transcriptFileName = `${getDateTimePrefix(recordingStartTime)}${customTitle ? ' ' + customTitle : ''} audio transcript.txt`,
                textBlob = new Blob([transcriptionText], { type: 'text/plain;charset=utf-8' });
          downloadBlob(textBlob, transcriptFileName);
        });
      }).catch(error => {
        console.error('Error accessing microphone:', error);
        alert("Microphone access denied or unavailable.");
      });
    }

    // Utility: Trigger download of a blob with a given file name.
    function downloadBlob(blob, fileName) {
      const a = document.createElement('a');
      a.href = URL.createObjectURL(blob);
      a.download = fileName;
      a.click();
      URL.revokeObjectURL(a.href);
    }

    // Utility: Remove the interim transcript span if it exists.
    function removeInterimTranscript() {
      const interim = document.getElementById('interim-transcript');
      if (interim) interim.remove();
    }

    // Utility: Generate a date/time prefix for file names.
    function getDateTimePrefix(time) {
      const date = new Date(time),
        year = date.getFullYear(),
        month = String(date.getMonth() + 1).padStart(2, '0'),
        day = String(date.getDate()).padStart(2, '0'),
        hours = String(date.getHours()).padStart(2, '0'),
        minutes = String(date.getMinutes()).padStart(2, '0'),
        seconds = String(date.getSeconds()).padStart(2, '0');
      return `${year}-${month}-${day}_${hours}-${minutes}-${seconds}`;
    }

    // Utility: Convert HTML content to plain text while preserving line breaks.
    function htmlToPlainText(html) {
      const tempDiv = document.createElement('div');
      tempDiv.innerHTML = html;
      function traverse(node) {
        let text = '';
        node.childNodes.forEach(child => {
          if (child.nodeType === Node.TEXT_NODE) text += child.textContent;
          else if (child.nodeType === Node.ELEMENT_NODE) {
            if (['BR', 'DIV', 'P'].includes(child.tagName)) text += '\n';
            text += traverse(child);
          }
        });
        return text;
      }
      return traverse(tempDiv).replace(/\n\s*\n/g, '\n\n').trim();
    }

    // Utility: Pad single-digit numbers with a leading zero.
    function padZero(num) { return num < 10 ? '0' + num : num; }

    // Updated beforeunload handler that works with the global isRecording variable
    window.addEventListener('beforeunload', event => {
      if (isRecording && exitWarningEnabled) {
        // Standard way to show a confirmation dialog before leaving the page
        const message = 'A recording is still in progress. Are you sure you want to leave?';
        event.preventDefault();
        event.returnValue = message; 
        return message;
      }
    });
  </script>
</body>

</html>