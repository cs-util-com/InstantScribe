<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Audio Recorder</title>
    <style>
        button {
            padding: 10px 15px;
            margin: 5px;
            background-color: #007bff;
            color: white;
            border: none;
            border-radius: 4px;
        }
        button:hover {
            background-color: #0056b3;
        }
        select {
            padding: 5px;
            margin: 5px;
        }
        #transcription-container {
            margin-top: 20px;
            padding: 10px;
            border: 1px solid #ccc;
            border-radius: 4px;
            max-width: 90%;
            max-width: 1200px;
        }
    </style>
</head>
<body>
    <button id="stop">Save Recording</button>
    <label for="language-select">Choose language:</label>
    <select id="language-select">
        <option value="en-US">English (US)</option>
        <option value="en-GB">English (UK)</option>
        <option value="de-DE">German</option>
        <option value="fr-FR">French</option>
        <option value="es-ES">Spanish</option>
        <option value="it-IT">Italian</option>
        <option value="ja-JP">Japanese</option>
        <option value="zh-CN">Chinese (Simplified)</option>
    </select>
    <h3>Transcription:</h3>
    <div id="transcription-container">
        <p id="transcription"></p>
    </div>

    <script>
        if (!window.MediaRecorder || !('webkitSpeechRecognition' in window || 'SpeechRecognition' in window)) {
            alert("Your browser does not support the required APIs for audio recording and transcription.");
        } else {
            navigator.mediaDevices.getUserMedia({ audio: true }).then(stream => {
                // Audio Recording
                const mediaRecorder = new MediaRecorder(stream, { mimeType: 'audio/webm' });
                const audioChunks = [];

                let isRecording = true;

                mediaRecorder.ondataavailable = event => audioChunks.push(event.data);
                mediaRecorder.onstop = () => {
                    const blobUrl = URL.createObjectURL(new Blob(audioChunks, { type: 'audio/webm' }));
                    const a = document.createElement('a');
                    a.href = blobUrl;
                    a.download = `${getDateTimePrefix()} audio recording.webm`;
                    a.click();
                    URL.revokeObjectURL(blobUrl);
                };

                mediaRecorder.start();
                const stop = document.getElementById('stop');
                stop.onclick = () => {
                    mediaRecorder.stop();
                    recognition.stop();
                    stop.style.display = 'none';
                    isRecording = false;

                    { // Also download the transcription as a txt file:
                        const blob = new Blob([transcriptionElement.innerText], { type: 'text/plain' });
                        const a = document.createElement('a');
                        a.href = URL.createObjectURL(blob);
                        a.download = `${getDateTimePrefix()} audio transcript.txt`;
                        a.click();
                        URL.revokeObjectURL(a.href);
                    }
                };

                // Automatic Transcription
                const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
                let recognition = new SpeechRecognition();
                recognition.continuous = true;
                recognition.interimResults = true;
                let transcriptionElement = document.getElementById('transcription');

                // Set default language to browser language
                const languageSelect = document.getElementById('language-select');
                recognition.lang = navigator.language || 'en-US';
                languageSelect.value = recognition.lang;

                // Variables to track silence
                let transcriptionText = '';
                let lastResultTime = Date.now();
                let lineBreakInserted = false;

                // Function to initialize recognition
                const initializeRecognition = () => {
                    recognition.onresult = function(event) {
                        lastResultTime = Date.now();
                        lineBreakInserted = false;
                        let final_transcript = '';
                        let interim_transcript = '';
                        for (let i = event.resultIndex; i < event.results.length; ++i) {
                            if (event.results[i].isFinal) {
                                final_transcript += event.results[i][0].transcript + ' ';
                            } else {
                                interim_transcript += event.results[i][0].transcript + ' ';
                            }
                        }
                        transcriptionText += final_transcript;
                        transcriptionElement.innerText = transcriptionText + interim_transcript;
                    };
                    recognition.onerror = event => {
                        console.error('Speech recognition error:', event.error);
                        initializeRecognition(); // Restart recognition when there was an error
                    };
                };

                // Initialize and start recognition
                initializeRecognition();
                recognition.start();

                // Set up interval to check for silence
                setInterval(() => {
                    if (Date.now() - lastResultTime > 1000 && !lineBreakInserted) {
                        transcriptionText += "\n";
                        transcriptionElement.innerText = transcriptionText;
                        lineBreakInserted = true;
                    }
                }, 500);

                // Update language when the dropdown value changes
                languageSelect.addEventListener('change', () => {
                    recognition.stop();
                    recognition = new SpeechRecognition();
                    recognition.continuous = true;
                    recognition.interimResults = true;
                    recognition.lang = languageSelect.value;
                    initializeRecognition();
                    recognition.start();
                });

                // Warn the user before leaving the page while recording
                window.addEventListener('beforeunload', (event) => {
                    if (isRecording) {
                        event.preventDefault();
                        event.returnValue = 'A recording is still in progress. Are you sure you want to leave?';
                    }
                });

            }).catch(error => {
                console.error(error);
                alert("Microphone access denied or unavailable.");
            });
        }

        // Function to get date and time prefix
        function getDateTimePrefix() {
            const now = new Date();
            const year = now.getFullYear();
            const month = String(now.getMonth() + 1).padStart(2, '0');
            const day = String(now.getDate()).padStart(2, '0');
            const hours = String(now.getHours()).padStart(2, '0');
            const minutes = String(now.getMinutes()).padStart(2, '0');
            const seconds = String(now.getSeconds()).padStart(2, '0');
            return `${year} ${month} ${day} ${hours}_${minutes}_${seconds}`;
        }
    </script>
</body>
</html>
